{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "77b7a4f5-ebd4-458b-8865-9cde5469cd0d",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "import random\n",
    "from collections import defaultdict\n",
    "\n",
    "import cv2\n",
    "import re\n",
    "\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import torch\n",
    "import html\n",
    "import gradio as gr\n",
    "\n",
    "import torchvision.transforms as T\n",
    "import torch.backends.cudnn as cudnn\n",
    "\n",
    "from minigpt4.common.config import Config\n",
    "\n",
    "from minigpt4.common.registry import registry\n",
    "from minigpt4.conversation.conversation import Conversation, SeparatorStyle, Chat\n",
    "\n",
    "# imports modules for registration\n",
    "from minigpt4.datasets.builders import *\n",
    "from minigpt4.models import *\n",
    "from minigpt4.processors import *\n",
    "from minigpt4.runners import *\n",
    "from minigpt4.tasks import *\n",
    "from minigpt4.conversation.conversation import CONV_VISION_minigptv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a5db262-8f7c-47cf-9605-840c49bd95f1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e12fdb1b-c215-48c7-8d53-81ef4bbaab82",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "02f6b2a4-56e6-44c2-baff-6ffd2b6e2ed9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing Chat\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "args_dict = {\n",
    "    \"cfg-path\": 'eval_configs/minigptv2_eval.yaml',\n",
    "    \"gpu-id\": 0,\n",
    "    \"options\": None\n",
    "}\n",
    "parser = argparse.ArgumentParser(description=\"Demo\")\n",
    "\n",
    "for arg_name, arg_value in args_dict.items():\n",
    "    parser.add_argument(f\"--{arg_name}\", default=arg_value)\n",
    "    \n",
    "args = parser.parse_args([])\n",
    "\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "cudnn.benchmark = False\n",
    "cudnn.deterministic = True\n",
    "\n",
    "print('Initializing Chat')\n",
    "# args = parse_args()\n",
    "cfg = Config(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "ec7769a7-9762-4e73-a5d9-22e21beb7b35",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<minigpt4.common.config.Config at 0x7fdb520f3910>"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f0235981-1013-494d-b913-302d4b2d01cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***********************\n",
      "LLAMA MODEL PATH: models/Meta-Llama-3-8B-Instruct\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at models/Meta-Llama-3-8B-Instruct were not used when initializing LlamaForCausalLM: ['layers.0.attention.wv.weight', 'layers.1.attention.wk.weight', 'layers.14.attention.wq.weight', 'layers.0.feed_forward.w1.weight', 'layers.9.attention.wq.weight', 'layers.21.attention.wk.weight', 'layers.8.attention_norm.weight', 'layers.10.feed_forward.w3.weight', 'layers.7.attention.wo.weight', 'layers.16.attention.wv.weight', 'layers.19.attention.wq.weight', 'layers.28.feed_forward.w1.weight', 'layers.0.feed_forward.w3.weight', 'layers.23.ffn_norm.weight', 'layers.30.attention.wo.weight', 'layers.27.attention.wk.weight', 'layers.20.ffn_norm.weight', 'layers.5.attention.wo.weight', 'layers.3.ffn_norm.weight', 'layers.13.feed_forward.w2.weight', 'layers.4.attention_norm.weight', 'layers.31.attention_norm.weight', 'layers.16.attention.wo.weight', 'layers.29.ffn_norm.weight', 'layers.14.feed_forward.w3.weight', 'layers.27.attention.wq.weight', 'layers.27.feed_forward.w1.weight', 'layers.25.attention.wv.weight', 'layers.15.feed_forward.w3.weight', 'layers.28.ffn_norm.weight', 'layers.28.attention.wv.weight', 'layers.1.feed_forward.w3.weight', 'layers.23.attention.wk.weight', 'layers.5.attention.wq.weight', 'layers.5.attention.wv.weight', 'layers.18.feed_forward.w2.weight', 'layers.1.attention.wq.weight', 'layers.5.feed_forward.w3.weight', 'layers.23.attention.wo.weight', 'layers.24.attention.wo.weight', 'layers.5.ffn_norm.weight', 'layers.9.attention.wv.weight', 'layers.12.ffn_norm.weight', 'layers.25.attention.wo.weight', 'layers.4.ffn_norm.weight', 'layers.16.attention_norm.weight', 'layers.20.attention.wv.weight', 'layers.11.attention.wo.weight', 'layers.23.attention.wv.weight', 'layers.26.feed_forward.w1.weight', 'layers.16.feed_forward.w3.weight', 'layers.21.ffn_norm.weight', 'layers.31.attention.wq.weight', 'layers.19.feed_forward.w3.weight', 'layers.7.attention_norm.weight', 'layers.17.attention.wq.weight', 'layers.19.ffn_norm.weight', 'layers.24.feed_forward.w2.weight', 'layers.4.attention.wk.weight', 'layers.6.attention.wq.weight', 'layers.18.attention.wk.weight', 'layers.6.feed_forward.w2.weight', 'layers.7.feed_forward.w1.weight', 'layers.18.attention.wq.weight', 'layers.10.ffn_norm.weight', 'layers.15.ffn_norm.weight', 'layers.22.attention_norm.weight', 'layers.27.attention_norm.weight', 'layers.8.attention.wq.weight', 'layers.8.feed_forward.w3.weight', 'layers.31.feed_forward.w2.weight', 'layers.25.attention_norm.weight', 'layers.8.feed_forward.w2.weight', 'layers.5.attention.wk.weight', 'layers.4.attention.wq.weight', 'layers.15.attention.wv.weight', 'layers.21.feed_forward.w1.weight', 'layers.3.feed_forward.w1.weight', 'layers.13.attention.wo.weight', 'layers.16.feed_forward.w2.weight', 'layers.18.ffn_norm.weight', 'layers.3.attention.wk.weight', 'layers.9.attention.wo.weight', 'layers.19.feed_forward.w2.weight', 'layers.20.attention_norm.weight', 'layers.9.ffn_norm.weight', 'layers.31.attention.wo.weight', 'layers.25.feed_forward.w1.weight', 'layers.29.attention.wv.weight', 'layers.0.attention.wq.weight', 'layers.20.attention.wq.weight', 'layers.17.attention.wk.weight', 'layers.2.ffn_norm.weight', 'layers.6.attention.wv.weight', 'layers.1.attention.wv.weight', 'layers.19.attention.wv.weight', 'layers.25.attention.wq.weight', 'layers.30.feed_forward.w1.weight', 'layers.15.attention.wq.weight', 'layers.24.feed_forward.w3.weight', 'layers.16.ffn_norm.weight', 'layers.27.ffn_norm.weight', 'layers.28.attention.wo.weight', 'layers.28.feed_forward.w3.weight', 'layers.4.attention.wv.weight', 'layers.28.feed_forward.w2.weight', 'layers.31.feed_forward.w1.weight', 'layers.23.attention.wq.weight', 'layers.10.attention.wv.weight', 'layers.4.feed_forward.w1.weight', 'layers.4.feed_forward.w3.weight', 'layers.13.attention.wq.weight', 'layers.8.attention.wv.weight', 'layers.24.attention.wv.weight', 'layers.11.attention.wk.weight', 'layers.17.attention.wo.weight', 'layers.3.attention_norm.weight', 'layers.25.feed_forward.w3.weight', 'layers.26.attention_norm.weight', 'layers.13.feed_forward.w3.weight', 'layers.0.attention.wo.weight', 'layers.3.attention.wv.weight', 'layers.12.feed_forward.w1.weight', 'layers.22.ffn_norm.weight', 'layers.23.feed_forward.w3.weight', 'layers.14.ffn_norm.weight', 'layers.10.feed_forward.w1.weight', 'layers.18.attention_norm.weight', 'tok_embeddings.weight', 'layers.8.attention.wk.weight', 'layers.24.attention.wq.weight', 'layers.6.feed_forward.w3.weight', 'layers.9.feed_forward.w3.weight', 'layers.14.feed_forward.w2.weight', 'layers.17.attention.wv.weight', 'layers.3.feed_forward.w2.weight', 'layers.11.feed_forward.w2.weight', 'layers.24.feed_forward.w1.weight', 'layers.2.feed_forward.w1.weight', 'layers.19.attention.wo.weight', 'layers.12.attention.wv.weight', 'layers.18.attention.wo.weight', 'layers.31.feed_forward.w3.weight', 'layers.0.feed_forward.w2.weight', 'layers.4.feed_forward.w2.weight', 'layers.13.feed_forward.w1.weight', 'layers.21.feed_forward.w2.weight', 'layers.1.attention_norm.weight', 'layers.16.feed_forward.w1.weight', 'layers.30.attention.wv.weight', 'layers.6.attention_norm.weight', 'layers.17.feed_forward.w1.weight', 'layers.6.attention.wk.weight', 'layers.23.feed_forward.w2.weight', 'layers.22.attention.wk.weight', 'layers.28.attention_norm.weight', 'layers.8.feed_forward.w1.weight', 'layers.20.feed_forward.w2.weight', 'layers.11.attention_norm.weight', 'layers.26.feed_forward.w2.weight', 'layers.21.attention.wo.weight', 'layers.30.attention_norm.weight', 'layers.5.feed_forward.w1.weight', 'layers.29.attention.wo.weight', 'layers.15.feed_forward.w1.weight', 'layers.17.attention_norm.weight', 'layers.20.feed_forward.w3.weight', 'layers.7.ffn_norm.weight', 'layers.27.attention.wv.weight', 'layers.30.attention.wk.weight', 'layers.2.attention.wq.weight', 'layers.17.ffn_norm.weight', 'layers.22.feed_forward.w2.weight', 'layers.25.attention.wk.weight', 'layers.7.attention.wk.weight', 'layers.12.attention.wo.weight', 'layers.26.attention.wk.weight', 'layers.26.attention.wo.weight', 'layers.29.attention.wk.weight', 'layers.12.feed_forward.w3.weight', 'layers.14.attention.wv.weight', 'output.weight', 'layers.29.feed_forward.w1.weight', 'layers.28.attention.wq.weight', 'layers.30.ffn_norm.weight', 'layers.6.ffn_norm.weight', 'layers.21.attention.wv.weight', 'layers.22.attention.wq.weight', 'layers.3.attention.wo.weight', 'layers.6.feed_forward.w1.weight', 'layers.1.ffn_norm.weight', 'layers.10.feed_forward.w2.weight', 'layers.26.feed_forward.w3.weight', 'layers.9.feed_forward.w1.weight', 'layers.0.ffn_norm.weight', 'layers.2.attention.wo.weight', 'layers.14.attention.wk.weight', 'layers.19.attention_norm.weight', 'layers.17.feed_forward.w2.weight', 'layers.22.feed_forward.w3.weight', 'layers.24.attention_norm.weight', 'layers.23.attention_norm.weight', 'layers.2.feed_forward.w3.weight', 'layers.15.attention.wk.weight', 'layers.16.attention.wk.weight', 'layers.24.ffn_norm.weight', 'layers.10.attention.wq.weight', 'layers.17.feed_forward.w3.weight', 'layers.19.feed_forward.w1.weight', 'layers.0.attention_norm.weight', 'layers.3.attention.wq.weight', 'layers.11.feed_forward.w1.weight', 'layers.13.ffn_norm.weight', 'layers.12.attention.wq.weight', 'layers.2.attention.wk.weight', 'layers.18.feed_forward.w1.weight', 'layers.23.feed_forward.w1.weight', 'layers.7.feed_forward.w2.weight', 'layers.6.attention.wo.weight', 'layers.4.attention.wo.weight', 'layers.14.feed_forward.w1.weight', 'layers.28.attention.wk.weight', 'layers.5.attention_norm.weight', 'layers.31.attention.wk.weight', 'layers.20.attention.wo.weight', 'layers.9.feed_forward.w2.weight', 'layers.1.attention.wo.weight', 'layers.22.attention.wo.weight', 'layers.0.attention.wk.weight', 'layers.2.attention_norm.weight', 'layers.9.attention.wk.weight', 'layers.14.attention.wo.weight', 'layers.26.attention.wv.weight', 'layers.2.attention.wv.weight', 'layers.18.feed_forward.w3.weight', 'layers.1.feed_forward.w2.weight', 'layers.10.attention.wo.weight', 'layers.27.feed_forward.w3.weight', 'layers.13.attention.wk.weight', 'layers.26.ffn_norm.weight', 'layers.15.attention_norm.weight', 'layers.29.attention_norm.weight', 'layers.31.attention.wv.weight', 'layers.16.attention.wq.weight', 'layers.9.attention_norm.weight', 'layers.29.attention.wq.weight', 'layers.15.attention.wo.weight', 'layers.26.attention.wq.weight', 'layers.7.attention.wq.weight', 'layers.7.attention.wv.weight', 'layers.27.feed_forward.w2.weight', 'layers.31.ffn_norm.weight', 'layers.25.ffn_norm.weight', 'layers.2.feed_forward.w2.weight', 'layers.11.ffn_norm.weight', 'layers.11.attention.wq.weight', 'layers.22.attention.wv.weight', 'layers.20.feed_forward.w1.weight', 'layers.11.attention.wv.weight', 'layers.3.feed_forward.w3.weight', 'layers.13.attention.wv.weight', 'layers.14.attention_norm.weight', 'layers.8.ffn_norm.weight', 'layers.10.attention_norm.weight', 'layers.12.attention.wk.weight', 'layers.1.feed_forward.w1.weight', 'layers.21.attention_norm.weight', 'layers.29.feed_forward.w3.weight', 'layers.7.feed_forward.w3.weight', 'layers.12.feed_forward.w2.weight', 'layers.24.attention.wk.weight', 'layers.30.feed_forward.w3.weight', 'layers.13.attention_norm.weight', 'layers.30.feed_forward.w2.weight', 'layers.25.feed_forward.w2.weight', 'layers.5.feed_forward.w2.weight', 'layers.18.attention.wv.weight', 'layers.30.attention.wq.weight', 'layers.21.attention.wq.weight', 'layers.12.attention_norm.weight', 'layers.21.feed_forward.w3.weight', 'layers.8.attention.wo.weight', 'layers.29.feed_forward.w2.weight', 'layers.11.feed_forward.w3.weight', 'layers.20.attention.wk.weight', 'layers.22.feed_forward.w1.weight', 'layers.27.attention.wo.weight', 'layers.19.attention.wk.weight', 'layers.10.attention.wk.weight', 'layers.15.feed_forward.w2.weight']\n",
      "- This IS expected if you are initializing LlamaForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing LlamaForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of LlamaForCausalLM were not initialized from the model checkpoint at models/Meta-Llama-3-8B-Instruct and are newly initialized: ['layers.12.mlp.gate_proj.weight', 'layers.11.mlp.up_proj.weight', 'layers.13.mlp.up_proj.weight', 'layers.12.post_attention_layernorm.weight', 'layers.1.mlp.up_proj.weight', 'layers.26.self_attn.k_proj.weight', 'layers.19.self_attn.v_proj.weight', 'layers.7.self_attn.v_proj.weight', 'layers.8.self_attn.rotary_emb.inv_freq', 'layers.22.self_attn.q_proj.weight', 'layers.26.mlp.up_proj.weight', 'layers.14.mlp.gate_proj.weight', 'layers.1.self_attn.k_proj.weight', 'layers.17.self_attn.q_proj.weight', 'layers.24.post_attention_layernorm.weight', 'layers.7.self_attn.k_proj.weight', 'embed_tokens.weight', 'layers.16.input_layernorm.weight', 'layers.23.post_attention_layernorm.weight', 'layers.6.mlp.down_proj.weight', 'layers.12.mlp.down_proj.weight', 'layers.19.mlp.up_proj.weight', 'layers.7.mlp.up_proj.weight', 'layers.10.mlp.down_proj.weight', 'layers.6.mlp.gate_proj.weight', 'layers.4.input_layernorm.weight', 'layers.7.self_attn.o_proj.weight', 'layers.15.post_attention_layernorm.weight', 'layers.24.self_attn.k_proj.weight', 'layers.24.input_layernorm.weight', 'layers.15.input_layernorm.weight', 'layers.29.mlp.gate_proj.weight', 'layers.21.self_attn.k_proj.weight', 'layers.0.self_attn.o_proj.weight', 'layers.19.self_attn.q_proj.weight', 'layers.23.self_attn.k_proj.weight', 'layers.27.post_attention_layernorm.weight', 'layers.31.self_attn.rotary_emb.inv_freq', 'layers.6.self_attn.o_proj.weight', 'layers.17.post_attention_layernorm.weight', 'layers.11.input_layernorm.weight', 'layers.17.input_layernorm.weight', 'layers.17.mlp.down_proj.weight', 'layers.22.post_attention_layernorm.weight', 'layers.7.mlp.gate_proj.weight', 'layers.22.self_attn.rotary_emb.inv_freq', 'layers.25.self_attn.q_proj.weight', 'layers.5.self_attn.rotary_emb.inv_freq', 'layers.1.self_attn.v_proj.weight', 'layers.21.self_attn.o_proj.weight', 'layers.8.self_attn.o_proj.weight', 'layers.14.self_attn.k_proj.weight', 'layers.16.self_attn.rotary_emb.inv_freq', 'layers.12.input_layernorm.weight', 'layers.3.self_attn.v_proj.weight', 'layers.7.post_attention_layernorm.weight', 'layers.13.mlp.down_proj.weight', 'layers.19.mlp.gate_proj.weight', 'layers.14.mlp.down_proj.weight', 'layers.18.mlp.gate_proj.weight', 'layers.19.mlp.down_proj.weight', 'layers.26.self_attn.v_proj.weight', 'layers.25.input_layernorm.weight', 'layers.1.self_attn.rotary_emb.inv_freq', 'layers.6.self_attn.k_proj.weight', 'layers.19.self_attn.rotary_emb.inv_freq', 'layers.25.self_attn.k_proj.weight', 'layers.3.mlp.gate_proj.weight', 'layers.9.self_attn.q_proj.weight', 'layers.8.input_layernorm.weight', 'layers.6.self_attn.rotary_emb.inv_freq', 'layers.3.post_attention_layernorm.weight', 'layers.24.self_attn.o_proj.weight', 'layers.12.mlp.up_proj.weight', 'layers.24.self_attn.q_proj.weight', 'layers.19.input_layernorm.weight', 'layers.0.self_attn.v_proj.weight', 'layers.29.self_attn.v_proj.weight', 'layers.29.mlp.up_proj.weight', 'layers.28.self_attn.q_proj.weight', 'layers.17.self_attn.k_proj.weight', 'layers.30.self_attn.q_proj.weight', 'layers.12.self_attn.k_proj.weight', 'layers.14.mlp.up_proj.weight', 'layers.14.self_attn.v_proj.weight', 'layers.30.mlp.up_proj.weight', 'layers.31.mlp.up_proj.weight', 'layers.6.post_attention_layernorm.weight', 'layers.30.mlp.gate_proj.weight', 'layers.5.mlp.gate_proj.weight', 'layers.10.self_attn.q_proj.weight', 'layers.18.self_attn.k_proj.weight', 'layers.10.input_layernorm.weight', 'layers.18.mlp.down_proj.weight', 'layers.15.mlp.gate_proj.weight', 'layers.22.mlp.gate_proj.weight', 'layers.31.self_attn.o_proj.weight', 'layers.22.input_layernorm.weight', 'layers.16.self_attn.o_proj.weight', 'layers.4.self_attn.o_proj.weight', 'layers.30.self_attn.o_proj.weight', 'layers.3.self_attn.q_proj.weight', 'layers.18.input_layernorm.weight', 'layers.31.post_attention_layernorm.weight', 'layers.28.input_layernorm.weight', 'layers.24.mlp.up_proj.weight', 'layers.26.post_attention_layernorm.weight', 'layers.16.self_attn.k_proj.weight', 'layers.27.self_attn.k_proj.weight', 'layers.9.input_layernorm.weight', 'layers.17.self_attn.o_proj.weight', 'layers.19.self_attn.o_proj.weight', 'layers.29.self_attn.k_proj.weight', 'layers.0.post_attention_layernorm.weight', 'layers.24.self_attn.rotary_emb.inv_freq', 'layers.29.mlp.down_proj.weight', 'layers.19.self_attn.k_proj.weight', 'layers.4.post_attention_layernorm.weight', 'layers.24.mlp.down_proj.weight', 'layers.0.mlp.up_proj.weight', 'layers.12.self_attn.q_proj.weight', 'layers.27.self_attn.v_proj.weight', 'layers.10.self_attn.o_proj.weight', 'layers.16.self_attn.q_proj.weight', 'layers.4.mlp.down_proj.weight', 'layers.31.input_layernorm.weight', 'layers.3.self_attn.o_proj.weight', 'layers.0.self_attn.rotary_emb.inv_freq', 'layers.16.mlp.down_proj.weight', 'layers.26.input_layernorm.weight', 'layers.6.self_attn.q_proj.weight', 'layers.6.self_attn.v_proj.weight', 'layers.23.self_attn.o_proj.weight', 'layers.10.self_attn.v_proj.weight', 'layers.10.mlp.gate_proj.weight', 'layers.14.input_layernorm.weight', 'layers.4.self_attn.rotary_emb.inv_freq', 'layers.2.self_attn.rotary_emb.inv_freq', 'layers.9.mlp.up_proj.weight', 'layers.20.self_attn.rotary_emb.inv_freq', 'layers.22.self_attn.v_proj.weight', 'layers.1.input_layernorm.weight', 'layers.24.mlp.gate_proj.weight', 'layers.8.self_attn.k_proj.weight', 'layers.0.input_layernorm.weight', 'layers.1.post_attention_layernorm.weight', 'layers.15.self_attn.q_proj.weight', 'layers.26.mlp.down_proj.weight', 'layers.21.mlp.down_proj.weight', 'layers.5.mlp.up_proj.weight', 'layers.14.self_attn.o_proj.weight', 'layers.31.self_attn.v_proj.weight', 'layers.15.mlp.up_proj.weight', 'layers.5.self_attn.q_proj.weight', 'layers.16.mlp.up_proj.weight', 'layers.17.mlp.up_proj.weight', 'layers.20.mlp.up_proj.weight', 'layers.14.self_attn.q_proj.weight', 'layers.23.input_layernorm.weight', 'layers.2.self_attn.o_proj.weight', 'layers.18.mlp.up_proj.weight', 'layers.28.mlp.down_proj.weight', 'layers.15.self_attn.v_proj.weight', 'layers.25.self_attn.rotary_emb.inv_freq', 'layers.13.self_attn.o_proj.weight', 'layers.21.self_attn.v_proj.weight', 'layers.11.self_attn.k_proj.weight', 'layers.12.self_attn.v_proj.weight', 'layers.31.self_attn.q_proj.weight', 'layers.28.self_attn.o_proj.weight', 'layers.27.self_attn.o_proj.weight', 'layers.27.self_attn.rotary_emb.inv_freq', 'layers.0.mlp.gate_proj.weight', 'layers.11.mlp.gate_proj.weight', 'layers.14.post_attention_layernorm.weight', 'layers.9.self_attn.rotary_emb.inv_freq', 'layers.28.mlp.gate_proj.weight', 'layers.24.self_attn.v_proj.weight', 'layers.26.self_attn.rotary_emb.inv_freq', 'layers.1.mlp.down_proj.weight', 'layers.31.mlp.gate_proj.weight', 'layers.10.self_attn.rotary_emb.inv_freq', 'layers.22.self_attn.o_proj.weight', 'layers.31.self_attn.k_proj.weight', 'layers.21.self_attn.q_proj.weight', 'layers.13.self_attn.k_proj.weight', 'layers.18.post_attention_layernorm.weight', 'layers.0.mlp.down_proj.weight', 'layers.21.self_attn.rotary_emb.inv_freq', 'layers.18.self_attn.o_proj.weight', 'layers.28.self_attn.k_proj.weight', 'layers.10.self_attn.k_proj.weight', 'layers.22.self_attn.k_proj.weight', 'lm_head.weight', 'layers.25.self_attn.o_proj.weight', 'layers.27.self_attn.q_proj.weight', 'layers.20.mlp.down_proj.weight', 'layers.26.self_attn.o_proj.weight', 'layers.20.post_attention_layernorm.weight', 'layers.30.self_attn.rotary_emb.inv_freq', 'layers.7.mlp.down_proj.weight', 'layers.17.self_attn.v_proj.weight', 'layers.29.self_attn.q_proj.weight', 'layers.16.mlp.gate_proj.weight', 'layers.25.mlp.gate_proj.weight', 'layers.10.post_attention_layernorm.weight', 'layers.13.post_attention_layernorm.weight', 'layers.9.mlp.gate_proj.weight', 'layers.8.post_attention_layernorm.weight', 'layers.16.post_attention_layernorm.weight', 'layers.28.self_attn.rotary_emb.inv_freq', 'layers.15.self_attn.rotary_emb.inv_freq', 'layers.23.mlp.down_proj.weight', 'layers.9.post_attention_layernorm.weight', 'layers.2.mlp.gate_proj.weight', 'layers.4.mlp.gate_proj.weight', 'layers.8.self_attn.v_proj.weight', 'layers.9.mlp.down_proj.weight', 'layers.14.self_attn.rotary_emb.inv_freq', 'layers.29.post_attention_layernorm.weight', 'layers.30.self_attn.k_proj.weight', 'layers.5.self_attn.v_proj.weight', 'layers.17.mlp.gate_proj.weight', 'layers.28.self_attn.v_proj.weight', 'layers.1.self_attn.o_proj.weight', 'layers.30.input_layernorm.weight', 'layers.2.self_attn.v_proj.weight', 'layers.11.self_attn.rotary_emb.inv_freq', 'layers.20.mlp.gate_proj.weight', 'layers.5.mlp.down_proj.weight', 'layers.6.mlp.up_proj.weight', 'layers.25.mlp.up_proj.weight', 'layers.27.input_layernorm.weight', 'layers.27.mlp.down_proj.weight', 'layers.28.post_attention_layernorm.weight', 'layers.3.mlp.down_proj.weight', 'layers.21.mlp.gate_proj.weight', 'layers.18.self_attn.rotary_emb.inv_freq', 'layers.2.self_attn.q_proj.weight', 'layers.4.mlp.up_proj.weight', 'layers.2.post_attention_layernorm.weight', 'layers.11.post_attention_layernorm.weight', 'layers.11.self_attn.q_proj.weight', 'layers.0.self_attn.k_proj.weight', 'layers.4.self_attn.q_proj.weight', 'layers.5.self_attn.o_proj.weight', 'layers.11.mlp.down_proj.weight', 'layers.5.post_attention_layernorm.weight', 'layers.20.self_attn.v_proj.weight', 'layers.3.input_layernorm.weight', 'layers.7.self_attn.rotary_emb.inv_freq', 'layers.30.self_attn.v_proj.weight', 'layers.16.self_attn.v_proj.weight', 'layers.8.mlp.up_proj.weight', 'layers.13.input_layernorm.weight', 'layers.6.input_layernorm.weight', 'layers.28.mlp.up_proj.weight', 'layers.13.self_attn.v_proj.weight', 'layers.4.self_attn.v_proj.weight', 'layers.3.mlp.up_proj.weight', 'layers.30.post_attention_layernorm.weight', 'layers.19.post_attention_layernorm.weight', 'layers.0.self_attn.q_proj.weight', 'layers.15.self_attn.k_proj.weight', 'layers.11.self_attn.o_proj.weight', 'layers.30.mlp.down_proj.weight', 'layers.2.mlp.down_proj.weight', 'layers.1.self_attn.q_proj.weight', 'layers.21.mlp.up_proj.weight', 'layers.21.input_layernorm.weight', 'layers.23.mlp.gate_proj.weight', 'layers.9.self_attn.k_proj.weight', 'layers.1.mlp.gate_proj.weight', 'layers.3.self_attn.rotary_emb.inv_freq', 'layers.25.self_attn.v_proj.weight', 'layers.26.self_attn.q_proj.weight', 'layers.8.mlp.gate_proj.weight', 'layers.31.mlp.down_proj.weight', 'layers.20.self_attn.q_proj.weight', 'layers.15.self_attn.o_proj.weight', 'layers.8.mlp.down_proj.weight', 'layers.20.input_layernorm.weight', 'layers.4.self_attn.k_proj.weight', 'layers.2.input_layernorm.weight', 'layers.12.self_attn.o_proj.weight', 'layers.9.self_attn.o_proj.weight', 'layers.13.self_attn.rotary_emb.inv_freq', 'layers.13.mlp.gate_proj.weight', 'layers.2.self_attn.k_proj.weight', 'layers.21.post_attention_layernorm.weight', 'layers.27.mlp.up_proj.weight', 'layers.25.post_attention_layernorm.weight', 'layers.12.self_attn.rotary_emb.inv_freq', 'layers.23.mlp.up_proj.weight', 'layers.23.self_attn.rotary_emb.inv_freq', 'layers.13.self_attn.q_proj.weight', 'layers.29.input_layernorm.weight', 'layers.22.mlp.down_proj.weight', 'layers.2.mlp.up_proj.weight', 'layers.10.mlp.up_proj.weight', 'layers.11.self_attn.v_proj.weight', 'layers.22.mlp.up_proj.weight', 'layers.23.self_attn.v_proj.weight', 'layers.17.self_attn.rotary_emb.inv_freq', 'layers.20.self_attn.k_proj.weight', 'layers.7.input_layernorm.weight', 'layers.9.self_attn.v_proj.weight', 'layers.7.self_attn.q_proj.weight', 'layers.8.self_attn.q_proj.weight', 'layers.5.input_layernorm.weight', 'layers.27.mlp.gate_proj.weight', 'layers.26.mlp.gate_proj.weight', 'layers.29.self_attn.rotary_emb.inv_freq', 'layers.18.self_attn.v_proj.weight', 'layers.15.mlp.down_proj.weight', 'layers.5.self_attn.k_proj.weight', 'layers.18.self_attn.q_proj.weight', 'layers.25.mlp.down_proj.weight', 'layers.23.self_attn.q_proj.weight', 'layers.29.self_attn.o_proj.weight', 'layers.3.self_attn.k_proj.weight', 'layers.20.self_attn.o_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 33554432 || all params: 8869122048 || trainable%: 0.3783286758080702\n",
      "LLAMA MODEL HIDDEN DIM: 6144\n",
      "LLAMA MODEL DIM: 1408\n",
      "LLAMA 3 MODEL HIDDEN DIM: 6144\n",
      "LLAMA MODEL HIDDEN DIM: 6144\n",
      "LLAMA MODEL DIM: 1408\n",
      "LLAMA 3 MODEL HIDDEN DIM: 6144\n",
      "LLAMA MODEL HIDDEN DIM: 6144\n",
      "LLAMA MODEL DIM: 1408\n",
      "LLAMA 3 MODEL HIDDEN DIM: 6144\n",
      "LLAMA MODEL HIDDEN DIM: 6144\n",
      "LLAMA MODEL DIM: 1408\n",
      "LLAMA 3 MODEL HIDDEN DIM: 6144\n",
      "LLAMA MODEL HIDDEN DIM: 6144\n",
      "LLAMA MODEL DIM: 1408\n",
      "LLAMA 3 MODEL HIDDEN DIM: 6144\n",
      "LLAMA MODEL HIDDEN DIM: 6144\n",
      "LLAMA MODEL DIM: 1408\n",
      "LLAMA 3 MODEL HIDDEN DIM: 6144\n",
      "LLAMA MODEL HIDDEN DIM: 6144\n",
      "LLAMA MODEL DIM: 1408\n",
      "LLAMA 3 MODEL HIDDEN DIM: 6144\n",
      "LLAMA MODEL HIDDEN DIM: 6144\n",
      "LLAMA MODEL DIM: 1408\n",
      "LLAMA 3 MODEL HIDDEN DIM: 6144\n",
      "LLAMA MODEL HIDDEN DIM: 6144\n",
      "LLAMA MODEL DIM: 1408\n",
      "LLAMA 3 MODEL HIDDEN DIM: 6144\n",
      "LLAMA MODEL HIDDEN DIM: 6144\n",
      "LLAMA MODEL DIM: 1408\n",
      "LLAMA 3 MODEL HIDDEN DIM: 6144\n",
      "LLAMA MODEL HIDDEN DIM: 6144\n",
      "LLAMA MODEL DIM: 1408\n",
      "LLAMA 3 MODEL HIDDEN DIM: 6144\n",
      "LLAMA MODEL HIDDEN DIM: 6144\n",
      "LLAMA MODEL DIM: 1408\n",
      "LLAMA 3 MODEL HIDDEN DIM: 6144\n",
      "LLAMA MODEL HIDDEN DIM: 6144\n",
      "LLAMA MODEL DIM: 1408\n",
      "LLAMA 3 MODEL HIDDEN DIM: 6144\n",
      "LLAMA MODEL HIDDEN DIM: 6144\n",
      "LLAMA MODEL DIM: 1408\n",
      "LLAMA 3 MODEL HIDDEN DIM: 6144\n",
      "LLAMA MODEL HIDDEN DIM: 6144\n",
      "LLAMA MODEL DIM: 1408\n",
      "LLAMA 3 MODEL HIDDEN DIM: 6144\n",
      "LLAMA MODEL HIDDEN DIM: 6144\n",
      "LLAMA MODEL DIM: 1408\n",
      "LLAMA 3 MODEL HIDDEN DIM: 6144\n",
      "LLAMA MODEL HIDDEN DIM: 6144\n",
      "LLAMA MODEL DIM: 1408\n",
      "LLAMA 3 MODEL HIDDEN DIM: 6144\n",
      "LLAMA MODEL HIDDEN DIM: 6144\n",
      "LLAMA MODEL DIM: 1408\n",
      "LLAMA 3 MODEL HIDDEN DIM: 6144\n",
      "LLAMA MODEL HIDDEN DIM: 6144\n",
      "LLAMA MODEL DIM: 1408\n",
      "LLAMA 3 MODEL HIDDEN DIM: 6144\n",
      "LLAMA MODEL HIDDEN DIM: 6144\n",
      "LLAMA MODEL DIM: 1408\n",
      "LLAMA 3 MODEL HIDDEN DIM: 6144\n",
      "LLAMA MODEL HIDDEN DIM: 6144\n",
      "LLAMA MODEL DIM: 1408\n",
      "LLAMA 3 MODEL HIDDEN DIM: 6144\n",
      "LLAMA MODEL HIDDEN DIM: 6144\n",
      "LLAMA MODEL DIM: 1408\n",
      "LLAMA 3 MODEL HIDDEN DIM: 6144\n",
      "LLAMA MODEL HIDDEN DIM: 6144\n",
      "LLAMA MODEL DIM: 1408\n",
      "LLAMA 3 MODEL HIDDEN DIM: 6144\n",
      "LLAMA MODEL HIDDEN DIM: 6144\n",
      "LLAMA MODEL DIM: 1408\n",
      "LLAMA 3 MODEL HIDDEN DIM: 6144\n",
      "LLAMA MODEL HIDDEN DIM: 6144\n",
      "LLAMA MODEL DIM: 1408\n",
      "LLAMA 3 MODEL HIDDEN DIM: 6144\n",
      "LLAMA MODEL HIDDEN DIM: 6144\n",
      "LLAMA MODEL DIM: 1408\n",
      "LLAMA 3 MODEL HIDDEN DIM: 6144\n",
      "LLAMA MODEL HIDDEN DIM: 6144\n",
      "LLAMA MODEL DIM: 1408\n",
      "LLAMA 3 MODEL HIDDEN DIM: 6144\n",
      "LLAMA MODEL HIDDEN DIM: 6144\n",
      "LLAMA MODEL DIM: 1408\n",
      "LLAMA 3 MODEL HIDDEN DIM: 6144\n",
      "LLAMA MODEL HIDDEN DIM: 6144\n",
      "LLAMA MODEL DIM: 1408\n",
      "LLAMA 3 MODEL HIDDEN DIM: 6144\n",
      "LLAMA MODEL HIDDEN DIM: 6144\n",
      "LLAMA MODEL DIM: 1408\n",
      "LLAMA 3 MODEL HIDDEN DIM: 6144\n",
      "LLAMA MODEL HIDDEN DIM: 6144\n",
      "LLAMA MODEL DIM: 1408\n",
      "LLAMA 3 MODEL HIDDEN DIM: 6144\n",
      "LLAMA MODEL HIDDEN DIM: 6144\n",
      "LLAMA MODEL DIM: 1408\n",
      "LLAMA 3 MODEL HIDDEN DIM: 6144\n",
      "LLAMA MODEL HIDDEN DIM: 6144\n",
      "LLAMA MODEL DIM: 1408\n",
      "LLAMA 3 MODEL HIDDEN DIM: 6144\n",
      "LLAMA MODEL HIDDEN DIM: 6144\n",
      "LLAMA MODEL DIM: 1408\n",
      "LLAMA 3 MODEL HIDDEN DIM: 6144\n",
      "LLAMA MODEL HIDDEN DIM: 6144\n",
      "LLAMA MODEL DIM: 1408\n",
      "LLAMA 3 MODEL HIDDEN DIM: 6144\n",
      "LLAMA MODEL HIDDEN DIM: 6144\n",
      "LLAMA MODEL DIM: 1408\n",
      "LLAMA 3 MODEL HIDDEN DIM: 6144\n",
      "LLAMA MODEL HIDDEN DIM: 6144\n",
      "LLAMA MODEL DIM: 1408\n",
      "LLAMA 3 MODEL HIDDEN DIM: 6144\n",
      "LLAMA MODEL HIDDEN DIM: 6144\n",
      "LLAMA MODEL DIM: 1408\n",
      "LLAMA 3 MODEL HIDDEN DIM: 6144\n",
      "LLAMA MODEL HIDDEN DIM: 6144\n",
      "LLAMA MODEL DIM: 1408\n",
      "LLAMA 3 MODEL HIDDEN DIM: 6144\n",
      "Position interpolate from 16x16 to 32x32\n",
      "Load Minigpt-4-LLM Checkpoint: /work/pi_donghyunkim_umass_edu/hochul/emnlp/kthk/MiniGPT-4/models/minigptv2_checkpoint.pth\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda:{}'.format(args.gpu_id)\n",
    "\n",
    "model_config = cfg.model_cfg\n",
    "# model_config.device_8bit = args.gpu_id\n",
    "model_cls = registry.get_model_class(model_config.arch)\n",
    "model = model_cls.from_config(model_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "40993d28-8429-4ddf-8bc4-d81526c4511b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.to(device)\n",
    "\n",
    "vis_processor_cfg = cfg.datasets_cfg.cc_sbu_align.vis_processor.train\n",
    "#['blip2_image_eval', 'blip2_image_train', 'blip_caption']\n",
    "vis_processor = registry.get_processor_class('blip2_image_train').from_config(vis_processor_cfg)\n",
    "\n",
    "model = model.eval()\n",
    "\n",
    "# CONV_VISION = Conversation(\n",
    "#     system=\"\",\n",
    "#     roles=(r\"<s>[INST] \", r\" [/INST]\"),\n",
    "#     messages=[],\n",
    "#     offset=2,\n",
    "#     sep_style=SeparatorStyle.SINGLE,\n",
    "#     sep=\"\",\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "b28337bc-21d8-42a8-bfa4-3897cdf12c7f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['blip2_image_eval', 'blip2_image_train', 'blip_caption']"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "registry.list_processors()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "93c595a3-e7cb-4b71-9b06-cc008f091871",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': 'blip2_image_eval', 'image_size': 448}"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vis_processor_cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "3a52e14f-2f6d-4dfc-82ae-69e33c45184e",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c3a9e0f8-8a42-4c7f-816b-1d379b273da5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'data_type': 'images', 'build_info': {'storage': '/work/pi_donghyunkim_umass_edu/hochul/emnlp/kthk/MiniGPT-4/minigpt4/configs/datasets/cc_sbu_align/'}, 'vis_processor': {'train': {'name': 'blip2_image_eval', 'image_size': 448}}, 'text_processor': {'train': {'name': 'blip_caption'}}}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cfg.datasets_cfg.cc_sbu_align"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "dd56c6d3-efa4-48ee-bcde-368ba876604f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MiniGPTv2(\n",
       "  (llama_model): PeftModelForCausalLM(\n",
       "    (base_model): LoraModel(\n",
       "      (model): LlamaForCausalLM(\n",
       "        (model): LlamaModel(\n",
       "          (embed_tokens): Embedding(128256, 4096, padding_idx=0)\n",
       "          (layers): ModuleList(\n",
       "            (0-31): 32 x LlamaDecoderLayer(\n",
       "              (self_attn): LlamaAttention(\n",
       "                (q_proj): Linear8bitLt(\n",
       "                  in_features=4096, out_features=4096, bias=False\n",
       "                  (lora_dropout): Dropout(p=0.05, inplace=False)\n",
       "                  (lora_A): Linear(in_features=4096, out_features=64, bias=False)\n",
       "                  (lora_B): Linear(in_features=64, out_features=4096, bias=False)\n",
       "                )\n",
       "                (k_proj): Linear8bitLt(in_features=4096, out_features=4096, bias=False)\n",
       "                (v_proj): Linear8bitLt(\n",
       "                  in_features=4096, out_features=4096, bias=False\n",
       "                  (lora_dropout): Dropout(p=0.05, inplace=False)\n",
       "                  (lora_A): Linear(in_features=4096, out_features=64, bias=False)\n",
       "                  (lora_B): Linear(in_features=64, out_features=4096, bias=False)\n",
       "                )\n",
       "                (o_proj): Linear8bitLt(in_features=4096, out_features=4096, bias=False)\n",
       "                (rotary_emb): LlamaRotaryEmbedding()\n",
       "              )\n",
       "              (mlp): LlamaMLP(\n",
       "                (gate_proj): Linear8bitLt(in_features=4096, out_features=14336, bias=False)\n",
       "                (down_proj): Linear8bitLt(in_features=14336, out_features=4096, bias=False)\n",
       "                (up_proj): Linear8bitLt(in_features=4096, out_features=14336, bias=False)\n",
       "                (act_fn): SiLUActivation()\n",
       "              )\n",
       "              (input_layernorm): LlamaRMSNorm()\n",
       "              (post_attention_layernorm): LlamaRMSNorm()\n",
       "            )\n",
       "          )\n",
       "          (norm): LlamaRMSNorm()\n",
       "        )\n",
       "        (lm_head): CastOutputToFloat(\n",
       "          (0): Linear(in_features=4096, out_features=128256, bias=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (visual_encoder): VisionTransformer(\n",
       "    (patch_embed): PatchEmbed(\n",
       "      (proj): Conv2d(3, 1408, kernel_size=(14, 14), stride=(14, 14))\n",
       "    )\n",
       "    (pos_drop): Dropout(p=0.0, inplace=False)\n",
       "    (blocks): ModuleList(\n",
       "      (0-38): 39 x Block(\n",
       "        (norm1): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=1408, out_features=4224, bias=False)\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=1408, out_features=1408, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (drop_path): Identity()\n",
       "        (norm2): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=1408, out_features=6144, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (fc2): Linear(in_features=6144, out_features=1408, bias=True)\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (ln_vision): LayerNorm((1408,), eps=1e-05, elementwise_affine=True)\n",
       "  (llama_proj): Linear(in_features=5632, out_features=4096, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "07094fff-5cac-4f08-ab39-729b49be8d07",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path = './dataset/dog_rb/images_high/0_01-garcia-stitched_10382.jpg'\n",
    "image_path = './examples_v2/float.png'\n",
    "image = Image.open(image_path).convert('RGB')\n",
    "image2 = Image.open(image_path).convert('RGB')\n",
    "image = vis_processor(image)\n",
    "question = \"what\"\n",
    "question = f\"Describe the scene\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "d982c429-4775-41c0-a9d7-4eea7b7e91f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 448, 448])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "1dec272b-13e1-486a-a8c7-dcfe34ebf8e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "image = image.unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "7e631f60-2edf-4860-ac3c-94f7d5a9b0f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_texts(texts, conv_temp):\n",
    "    convs = [conv_temp.copy() for _ in range(len(texts))]\n",
    "    [conv.append_message(\n",
    "        conv.roles[0], '<Img><ImageHere></Img> {}'.format(text)) for conv, text in zip(convs, texts)]\n",
    "    [conv.append_message(conv.roles[1], None) for conv in convs]\n",
    "    texts = [conv.get_prompt() for conv in convs]\n",
    "    return texts\n",
    "conv_temp = CONV_VISION_minigptv2.copy()\n",
    "# conv_temp.system = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "9a77723b-0b81-42a4-8c97-632ac2694c8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = prepare_texts([question], conv_temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "359409fb-023c-45f3-9451-103d7c9eed6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = f\"<Img><ImageHere></Img> [vqa] Based on the image, respond to this question with a short answer: Are there any cars nearby?'\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1927e0ae-4401-4bd8-85b7-cf9735c3a339",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000\">╭─────────────────────────────── </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Traceback </span><span style=\"color: #bf7f7f; text-decoration-color: #bf7f7f; font-weight: bold\">(most recent call last)</span><span style=\"color: #800000; text-decoration-color: #800000\"> ────────────────────────────────╮</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">&lt;module&gt;</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1</span>                                                                                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>1 model.generate(image, text, max_new_tokens=<span style=\"color: #0000ff; text-decoration-color: #0000ff\">100</span>, do_sample=<span style=\"color: #0000ff; text-decoration-color: #0000ff\">False</span>,temperature = <span style=\"color: #0000ff; text-decoration-color: #0000ff\">1</span>,             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">2 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>top_p = <span style=\"color: #0000ff; text-decoration-color: #0000ff\">0.9</span>)                                                                         <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">3 </span>                                                                                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">╰──────────────────────────────────────────────────────────────────────────────────────────────────╯</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold\">NameError: </span>name <span style=\"color: #008000; text-decoration-color: #008000\">'image'</span> is not defined\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[31m╭─\u001b[0m\u001b[31m──────────────────────────────\u001b[0m\u001b[31m \u001b[0m\u001b[1;31mTraceback \u001b[0m\u001b[1;2;31m(most recent call last)\u001b[0m\u001b[31m \u001b[0m\u001b[31m───────────────────────────────\u001b[0m\u001b[31m─╮\u001b[0m\n",
       "\u001b[31m│\u001b[0m in \u001b[92m<module>\u001b[0m:\u001b[94m1\u001b[0m                                                                                    \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m1 model.generate(image, text, max_new_tokens=\u001b[94m100\u001b[0m, do_sample=\u001b[94mFalse\u001b[0m,temperature = \u001b[94m1\u001b[0m,             \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m2 \u001b[0m\u001b[2m│   │   \u001b[0mtop_p = \u001b[94m0.9\u001b[0m)                                                                         \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m3 \u001b[0m                                                                                             \u001b[31m│\u001b[0m\n",
       "\u001b[31m╰──────────────────────────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n",
       "\u001b[1;91mNameError: \u001b[0mname \u001b[32m'image'\u001b[0m is not defined\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model.generate(image, text, max_new_tokens=100, do_sample=False,temperature = 1,\n",
    "        top_p = 0.9)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
